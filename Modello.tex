\chapter{Il modello}
\section{Modello Narmax}
Grazie all’uso di un modello esplicito di rumore, il modello NARMAX è in grado
di modellare l’effetto di rumori non bianchi, correlati a causa delle nonlinearità del
sistema.
Un sistema SISO tempo discreto può essere rappresentato da un modello NARMAX
descritto da
\begin{equation}
y_t=f(y_{t-1},\dots, y_{t-n_y},y_{t-1},\dots, y_{t-n_y},e_{t-1},\dots, e_{t-n_e})+e_t
\label{narmax}
\end{equation}
dove si assume che $f(\cdot)$ sia una funzione non lineare sconosciuta, $u_t\in\mathbb{R}$ e $y_t\in\mathbb{R}$ siano gli ingressi e uscite del sistema e $e_t\in\mathbb{R}$ denota un termine di rumore estratto dalla distribuzione normale $\normal{0}{\sigma_e^2}$.\\
Gli ordini della dinamica sono rispettivamente $n_u,n_y,n_e$.
Decomponiamo la funzione f (·) in una somma di funzioni di base polinomiali e
riesprimiamo la \ref{narmax} come combiaione di termini di processo e di rumore,
\begin{equation}
y_t=\sum_{j=1}^{M_p}\left(a_j y_{t-	\delta_{y,j}}^{k_{y,j}} u_{t-	\delta_{u,j}}^{k_{u,j}}\right)+
\sum_{j=1}^{M_e}\left(b_j y_{t-	\delta_{y,j}}^{k_{y,j}} u_{t-	\delta_{u,j}}^{k_{u,j}}
e_{t-	\delta_{e,j}}^{k_{e,j}}\right)+e_t
\end{equation}

dove il modello di processo è composto da $M_p$ monomi combinazione di soli termini
di uscita e di rumore e il modello di rumore è composto da $M_e$ monomi combinazione
di ingresso, uscita e rumore. Con $k_{y,j}$ , $k_{u,j}$ , $k_{e,j}\in N$ si indicano le potenze che compaiono nei monomi, con $\delta_{ y,j}$ , $\delta_{ e,j}\in \mathbb{N}-\{0\}$ e
$\delta_{ u,j}\in \mathbb{N}$ il ritardo delle varie grandezze.\\
Si ha quindi una forma pseudo-lineare dell'equazione di uscita
al passo attuale che per comodità è stata spezzata nella somma di due regressioni, una relativa al processo e una relativa al rumore
\begin{equation}
y_t=\phi_P(t,u,y)^T\ak+\phi_N(t,u,y,e)^T\cdot\bq+e_t
\end{equation}
In questa descrizione la mappa di uscita è lineare mentre tutta la non linearità è inserita negli elementi del regressore.\\
Dal modello si ottiene la formula dello stimatore
\begin{equation}
y_t=\phi_P(t,u,y)^T\cdot\ak+\phi_N(t,u,y,\epsilon(t,u,y,\ak,\bq))^T\cdot\bq
\end{equation} 

Si noti che nel modello teorico la non linearità è imputabile solo alla natura della funzione $f(\cdot)$ mentre nella espressione dello stimatore si aggiunge una ulteriore fonte di non linearità dovuta al fatto che la realizzazione del processo bianco $e(t)$ non è nota
e dunque si sostituisce con la realizzazione del residuo $\epsilon(t) $ che è funzione del vettore dei parametri.\\
Se si considera una finestra di campioni lunga $w$ per l'dentificazione si avranno a disposizione altrettante equazioni di regressione che per comodità possono essere impilate in una equazione vettoriale


\begin{equation}
\begin{pmatrix}
y(t) \\ 
y(t-1) \\ 
y(t-2) \\ 
\vdots \\ 
y(t-w+1)
\end{pmatrix} =\begin{pmatrix}
\phi_P(t,u,y)^T \\ 
\phi_P(t-1,u,y)^T \\ 
\phi_P(t-2,u,y)^T \\ 
\vdots \\ 
\phi_P(t-w+1,u,y)^T
\end{pmatrix}\ak+\begin{pmatrix}
\phi_N(t,u,y,e)^T \\ 
\phi_N(t-1,u,y,e)^T \\ 
\phi_N(t-2,u,y,e)^T \\ 
\vdots \\ 
\phi_N(t-w+1,u,y,e)^T
\end{pmatrix}\bq+\begin{pmatrix}
e(t) \\ 
e(t-1) \\ 
e(t-2) \\ 
\vdots \\ 
e(t-w+1)
\end{pmatrix}
\end{equation}
indicata compattamente come
\begin{equation}
\y=\pk\ak+\eq\bq+e \label{reg}
\end{equation}

\section{Densità di probabilità a posteriori del modello}
Dal teorema di Bayes
\begin{equation}
p(k, q, \pk , \eq, \ak , \bq |\y) \propto p(\y|k, q, \pk ,\eq , \ak, \bq , \ve )p(k, q, \pk, \eq,\ak,\bq ) \label{post}
\end{equation}
\subsection{Likehood}
Se il rumore è gaussiano a media nulla anche la verosimiglianza è una gaussiana, funzione del vettore dei residui, di stessa varianza. Infatti come si deduce dall’equazione
di regressione (\ref{reg}) si ha
\begin{equation}
p(\y|k,\,\pk,eq,\ak,\bq,\ve)=p(\pk\ak+\eq\bq+\epsilon|k,q,\pk,\eq,\ak,\bq,\ve)=p(\epsilon|k,q,\pk,\eq,\ak,\bq,\ve)
\end{equation}
e supponendo di aver identificato correttamente i termini e i parametri in modo tale
da avere come errore residuo (ineliminabile) il rumore di misura si ha
\begin{equation}
p(\textbf{e}|k, q, \pk , \eq , \ak , \bq , \ve ) = p(\epsilon |k, q, \pk , \eq , \ak , \bq , \ve )=\frac{1}{\sqrt{2\pi\ve}^N}exp\left(-\frac{1}{2\ve}\epsilon^T\epsilon\right)
\end{equation}
dunque in definitiva
\begin{equation}
p(\y|k,\,\pk,eq,\ak,\bq,\ve)=\frac{1}{\sqrt{2\pi\ve}^N}exp\left(-\frac{1}{2\ve}\epsilon^T\epsilon\right)
\end{equation}
con
\begin{equation}
\epsilon=\y-\pk\ak-\eq\bq
\end{equation}
\section{Scelta delle distribuzioni a priori e iperparametri}
In assenza di preferenze, la scelta delle distribuzioni a priori solitamente ricade su
distribuzioni poco informative ovvero densità con un supporto ampio e varianza
grande.\\
Per aumentare la flessibilità si adotta una struttura gerarchica in cui gli stessi
parametri delle distribuzioni a priori sono a loro volta realizzazioni di variabili aleatorie. Per convenienza si sceglie di rappresentare i parametri con una distribuzione a
prori coniugata della likehood.
\section{Il concetto di distribuzioni coniugate}
Sia X una variabile che modella il sistema estratta da una distribuzione $p(X=x|\zeta)$.
Supponiamo cioè che il profilo della distribuzione sia dipendente dal parametro $\zeta$.\\
Per la variabile aleatoria $X$ la $p(X|\zeta)$ rappresenta la probabilità a priori rispetto ai dati.\\
Se si concentra l'attenzione sul parametro e si assume che anche esso abbia natura aleatoria la stessa $p(X|\zeta)$ rappresenta invece la likehood  del parametro $\zeta$ rispetto alle realizzazioni della $X$. 
Per la variabile che rappreseta il parametro vale quindi la regola di Bayes
\begin{equation}
p(\zeta|X)\propto p(X|\zeta)p(\zeta)
\end{equation}
La distribuzione $p(X|\zeta)$ è quindi un operatore che mappa una distribuzione in un'altra. E’ possibile per tale operatore trovare una sorta di \emph{autofunzione}, nel senso
 lato di distribuzione appartenente a una certa famiglia (normale, uniforme, poisson etc ) che viene mappata dall’operatore in un’altra funzione ancora appartenete
alla medesima famiglia.
Data una certa likehood le sue \emph{autofunzioni} in questo senso, vengono dette distribuzioni coniugate.

Se i prior sono scelti tra le distribuzioni coniugate, quando (mediante l’applicazione
dell’operatore likehood) evolvono non cambiano la forma d’onda ma solo i parametri
che la descrivono. Quindi la descrizione di una dinamica su uno spazio funzionale a
infinite dimensioni viene rappresentata dalla dinamica in uno spazio il cui numero di
dimensioni è finito e ridotto. (Ad esempio l’evoluzione di una gaussiana può essere
rappresentata concisamente dall’evoluzione del suo valor medio e della sua varianza).\\
Se  i prior non sono scelti tra le distribuzioni coniugate, invece, ad ogni applicazione
dell’operatore likehood cambia l’intera forma d’onda della distribuzione diventando
impossibile da descrivere analiticamente.
\subsection{Probabilità del modello a priori}
Modificando leggermente la (\ref{post}) per tenere conto anche degli iperparametri (descritti nel dettaglio in questa sezione) si ha
\begin{equation}
p(k,q,\pk,eq,\ak,\bq,\va,\vb,\la,\lb|\y)\propto
p(\y|k,q,\pk,ak,\bq,ve)
p(k,q,\pk,\eq,\ak,\bq,\va,\vb,\la,\lb)
\end{equation}
fattorizzabile come
\begin{align}
p(k,q,\pk,eq,\ak,\bq,\va,\vb,\la,\lb|\y)= & p(\ak|\pk,k,\va)p(k|\la)p(\pk)p(\la)p(\va)\times \nonumber\\
& p(\bq|\eq,q,\vb)p(q|\lb)p(\eq)p(\lb)p(\vb)
\end{align}
Di seguito vediamo nel dettaglio come sono state modellate le distribuzioni a priori
\subsection*{Vettore dei parametri dei termini di modello}
Il vettore di parametri $\ak$ si assume distribuito come una gaussiana multidimensionale isotropica a media nulla ovvero
\begin{equation}
p(\ak|k, \pk , \va ) = p(\ak |k, \va ) \sim \normal{\textbf{0}}{\va\textbf{I}}
\end{equation}
Questa scelta implica che la varianza della gaussiana sia distribuita come una gamma
inversa in modo da essere coniugata. La distribuzione gamma inversa è definita come
segue
\begin{equation}
\mathcal{I}\mathcal{G}(x|\alpha,\beta)= x^{-\alpha-1}\exp\left(-\frac{\beta}{x}\right)
\end{equation}
Di seguito si prova che la distribuzione gamma inversa è effettivamente coniugata
della gaussiana per l’iperparametro varianza\\
\textit{Proof.} è sufficiente applicare la regola di Bayes e ottenere ancora una gamma inversa
\begin{align*}
p(\va|k,\ak) & \propto p(\ak,k,\va)\cdot p(\va)\\
			 &\propto   \frac{1}{\sqrt{\va}^k}exp\left(\frac{-\ak^T\ak}{2\va}\right)\sigma_a^{2(-\alpha_a-1)}exp\left(-\frac{\beta_a}{\va}\right)\\
			 &\propto  \sigma_a^{2(-\alpha_a-1-\frac{1}{2}k)}exp\left(-\frac{\beta_a+\frac{1}{2}\ak^T\ak}{\va}\right) \\ 
			 &\propto   \sigma_a^{2(-\alpha_a-1-\frac{1}{2}k)}exp\left(-\frac{\beta_a+\frac{1}{2}\ak^T\ak}{\va}\right) \\
			 &\propto \mathcal{I}\mathcal{G}(\va|\alpha+\frac{1}{2}k,\beta_a+\frac{1}{2}\ak^T\ak)	
\end{align*}
 \begin{flushright}
 $\square$
 \end{flushright}

 \subsection*{Vettore dei parametri dei termini di rumore}
 La modellazione del vettore dei parametri dei termini di rumore è analoga a quella
trattata nella sottosezione precedente. Il vettore di parametri $\bq$ si assume distribuito come una gaussiana multidimensionale isotropica a media nulla
\begin{equation}
p(\bq |k, \eq , \vb ) = p(\bq |q, \vb ) \sim \normal{\textbf{0}}{\vb\textbf{I}}
\end{equation}

Questa scelta implica che la varianza della gaussiana sia distribuita come una gamma
inversa in modo da essere coniugata. Con dimostrazione del tutto analoga alla
sezione precedente si dimostra che la varianza della gaussiana deve essere una gamma
inversa.
\begin{align*}
p(\vb)=\mathcal{I}\mathcal{G}(\vb|\alpha_b,\beta_b)\Rightarrow
p(\vb|q,\bq) & \propto p(\bq,q,\vb)\cdot p(\vb)\\
	 &\propto \mathcal{I}\mathcal{G}(\vb|\alpha_b+\frac{1}{2}q,\beta_b+\frac{1}{2}\bq^T\bq)	
\end{align*}

\subsection*{Numero di termini di processo}
La probabilità a priori del numero di termini di processo è rappresentata da una
distribuzione di Poisson troncata, il parametro $\la$.
\begin{equation}
p(k|\la)=\frac{\frac{\la^k}{k!}}{\sum_{i=0}^N\frac{\la^i}{i!}}
\end{equation}
Con N numero massimo di termini di processo.
Questo tipo di distribuzione è più indicato rispetto ad una distribuzione uniforme,
perchè l’iperparametro $\la$ può essere interpretato come il numero medio di termini
ipotizzando un numero di termini possibili sufficientemente alto infatti
\begin{equation}
\mathbb{E}[k|\la]=\frac{\sum_{k=0}^Nk\frac{\la^k}{k!}}{\sum_{i=0}^N\frac{\la^i}{i!}}=\frac{\sum_{k=1}^Nk\frac{\la^k}{k!}}{\sum_{i=0}^N\frac{\la^i}{i!}}=\la\frac{\sum_{j=0}^{N-1}k\frac{\la^j}{j!}}{\sum_{i=0}^N\frac{\la^i}{i!}}=
\la\frac{\sum_{j=0}^{N-1}k\frac{\la^j}{j!}}{\sum_{i=0}^{N-1}\frac{\la^i}{i!}+\frac{\la^N}{N!}}=\la\frac{1}{1+\frac{\frac{\la^N}{N!}}{\sum_{i=0}^{N-1}\frac{\la^i}{i!}}}
\end{equation}
si noti che
\begin{equation}
\lim_{N\rightarrow\infty}\frac{\frac{\la^N}{N!}}{\sum_{i=0}^{N-1}\frac{\la^i}{i!}}=0
\end{equation}
L’iperparametro $\la$ è estratto da una distribuzione gamma
\begin{equation}
p(\la)=\mathcal{G}\mathcal{A}(\la|\alpha_{\la},\beta_{la})=\la^{(\alpha_{\la}-1)}exp\left(-\frac{\la}{\beta_{\la}}\right)
\end{equation}
Di seguito dimostro che la distribuzione gamma è coniugata per il parametro della
poisson
\textit{Proof.}
\begin{align*}
p(\la|k)&\propto p(k|\la)p(\la)\\
&\frac{\la^k}{k!}\la^{(\alpha_{\la}-1)}exp\left(-\frac{\la}{\beta_{\la}}\right)\\
&\propto\frac{1}{k!}\mathcal{G}\mathcal{A}(\la|\alpha_{\la}+k,\beta_{\la})
\end{align*}
 \begin{flushright}
 $\square$
 \end{flushright}
 \subsection*{Numero di termini di rumore}
 Analogamente a quanto detto nella sezione precedente, il numero dei termini di
rumore è modellato come una variabile aleatoria Poisson troncata
\begin{equation}
p(q|\lb)=\frac{\frac{\lb^q}{q!}}{\sum_{i=0}^N\frac{\lb^i}{i!}}
\end{equation}
e il suo iperparametro $\lb$ è modellato come una variabile aleatoria gamma-distribuita
\begin{equation}
p(\lb)=\mathcal{G}\mathcal{A}(\lb|\alpha_{\lb}.\beta_{\lb})=\lb^{(\alpha_{\lb}-1)}exp\left(-\frac{\lb}{\beta_{\la}}\right)
\end{equation}
\subsection*{Matrice di regressione dei termini di processo e di
rumore}
La matrice di regressione dei termini è considerata una va aleatoria uniformemente
distribuita in modo che nessun termine di modello nè di rumore sia, a priori delle
misure, più probabile di altri
\begin{equation}
p(\pk)\propto 1
\end{equation}
\begin{equation}
p(\eq )\propto 1
\end{equation}
